https://raw.githubusercontent.com/infatoshi/me/refs/heads/main/mojo-course.png

# 3e8
- assemble a wormhole network
    - [wormhole](https://en.wikipedia.org/wiki/Wormhole)
    - ![](../assets/wormhole.png)
    - speculative
    - requires a lot of overhead transport to setup?
    - requires complete understanding of the wormhole is it even exists
    - use ASI to automate
- ion thrusters
    - [ion thruster](https://en.wikipedia.org/wiki/Ion_thruster)
    - ![](../assets/ionthruster.png)
    - need more energy on board to push electrons
- antimatter propulsion
    - [antimatter](https://en.wikipedia.org/wiki/Antimatter_rocket)
    - ![](../assets/antimatter-rocket.png)

# Energy
- [nuclear fusion](https://en.wikipedia.org/wiki/Nuclear_fusion)
    - tokamak
        - ![](../assets/nuclear-fusion.png)
        - [ITER project](https://www.iter.org/)
        - ![](../assets/fusion-reactor.gif)
    - railgun
        - ![](../assets/helion-fuser.png)
        - [Helion Energy](https://www.helionenergy.com/)
- [black hole](https://en.wikipedia.org/wiki/Black_hole) harvesting
    - ![](../assets/blackhole.gif)
    - ![](../assets/blackhole-math.png)
    - [black hole energy harvesting explained by Grok 3](https://grok.com/share/bGVnYWN5_1bf586af-ed84-46b9-af7d-03a183992e91)
- [solar](https://en.wikipedia.org/wiki/Solar_system) (during travel)
    - [how do solar panels work?](https://www.youtube.com/watch?v=xKxrkht7CpY&ab_channel=TED-Ed)
    - [solar panels by Grok 3](https://grok.com/share/bGVnYWN5_5a553dc0-8071-4b2c-916d-2e69142380eb)
    - ![](../assets/solar-panels.png)
    - ![](../assets/solar.png)
    - ![](../assets/solar-travel.png)
- energy transfer
    - electricity
        - superconductors
    
- energy storage
    - batteries
    - stars
    - black holes (to harvest later)

# Solving Intelligence
 - understanding consciousness
    - understand the brain
        - neuroscience + electro-chemisry
    - find aliens
        - long distance lens / communication
        - space travel and exploration near alien-like solar system clusters
            - cryogenic sleep
            - AI autopilot
            - sustainable energy for life support & propulsion (nuclear fusion)

- ASI
    - [the bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)
    - compute infrastructure
        - robot construction
        - automated chip design
            - ![](../assets/h100.png)
        - sustainable energy for compute
        - materials required for large clusters
            - asteroid mining
                - assembly theory level autonomy in the limit
                - on board energy source for propulsion
                - powerful AGI should be able to automate all of the non-physical aspects of this
                    - simulations

    - self-improving AI
        - alignment / safety
	       - what 2026 looks like? https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like
	       - https://situational-awareness.ai/
            - [situational awareness](https://github.com/infatoshi/situationalawareness)
                - [official](https://situational-awareness.ai/)
            - [lesswrong](https://www.lesswrong.com/)
            - https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ
            - [rich sutton's views on alignment/safety](https://www.youtube.com/watch?v=w177Ov-Y3gc&t=63s&ab_channel=PrinceTakamadoJapanCentre)
            - https://darioamodei.com/machines-of-loving-grace
            - mech interp
                - - [www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic interpretability quickstart guide](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide)
				- Alignmentforum.org
				- Tools
				   - TransformerLens
				   - Neuroscope
				- 200 different problems in mech interp as see in the post
				- Superposition and Polysemanticity
				   - Neuron polysemanticity is the observed phenomena that many neurons seem to fire (have large, positive activations) on multiple unrelated concepts. Superposition is a specific explanation for neuron (or attention head) polysemanticity, where a neural network represents more sparse features than there are neurons (or number of/dimension of attention heads) in near-orthogonal directions. 
				- Neel nanda pioneered this stuff [Neel Nanda](https://www.neelnanda.io/?offset=1601395744946&reversePaginate=true)
				- Einsum and einops to avoid bugs 
				- Induction heads
				   - The induction head pays attention to a token in the past (let's call it A).
				   - It then looks for the next occurrence of A in the sequence.
				   - If it finds A again, it predicts that the token that followed A the first time will likely follow A again.
				   - so sort of like how we "induce" something or do an induction proof. its not direct, but we can use our axioms to declare that since something "over there" happened, we infer something "here" also happened similarly. 
				   - example: "Also did you know..." - "Also lets jump back to..."
                - sparse autoencoders
                    - [original paper](https://arxiv.org/pdf/2309.08600)
                    - [grok 3's advice w/ deepthink](https://grok.com/share/bGVnYWN5_a72dd8c0-b5bf-4224-8a4d-3bc55a35fbba)
				- 
        - AGI
            - define AGI
                - ![](../assets/agi.png)
                - books
                    - [superintelligence - Nick Bostrom](https://dn790007.ca.archive.org/0/items/artificial-intelligence/2014_superintelligence_paths%2C_dangers%2C_strategies-Nick%20Bostrom.pdf)
                        - ![](../assets/superintelligence.png)
                    - [Life 3.0 - Max Tegmark](https://github.com/vhr1975/eBooks/blob/master/AI/Life%203.0%20Being%20Human%20in%20the%20Age%20of%20Artificial%20Intelligence%20-%20Jul%2031%2C%202018/Life_3.pdf)
                        - **Be sure to read "The Tale of the Omega Team"**
                        - ![](../assets/life30.png)
                - papers / articles
                    - [Levels of AGI: Operationalizing Progress on the Path to AGI](https://ar5iv.labs.arxiv.org/html/2311.02462)
                    - [What is Meant by AGI? On the Definition of Artificial General Intelligence](https://ar5iv.labs.arxiv.org/html/2404.10731)
                    - [Artiï¬cial General Intelligence:Concept, State of the Art, and Future Prospects](https://www.researchgate.net/publication/271390398_Artificial_General_Intelligence_Concept_State_of_the_Art_and_Future_Prospects)
                - modalities
                    - text
                    - image
                    - audio
                    - video
                    - touch
                - creativity
                  - [benching creativity](https://gwern.net/creative-benchmark)
                - embodiment (requiring a human by its side to operate)
                    - Minecraft AIs
	                   - - [Reddit - Dive into anything](https://www.reddit.com/r/ModdedMinecraft/comments/oydywj/forge_vs_fabric/)
						- ![](https://remnote-user-data.s3.amazonaws.com/AUa48LoXPVlbOeLuQXoQCmGu9TMWcsr0rlDw2iz8DVPMr-_9CfxrpzOUR0cR18qw1iipnJMselFV26OJ04wec8qNI5KArpCzcw3DP-aACtVnhKr9wAmmn_Dj7JE1ax9G.png)
						- ![](https://remnote-user-data.s3.amazonaws.com/ey2vl-IKVX60_XWOXr9EOqabeFZE9wWIyHA3FmYYVlkLPMbS1ZPQDIrM5U3ivgcc2HXRvie-AaWB-RhFiyrGba8h9i7-ikazhDBGZwFdfxGnG4oIuQiVRVgh_D3mRCHm.png)
						- ok so i should probably go with fabric if im not reverse engineering minerl or malmo (latest versions, top performance, modularity, works across all OSes)
						- the idea is to replicate minerl down to game ticks (forgot to define that i was aiming for this from the start). other non-perfectly synced options work but it wont make me feel complete.
						- [GitHub - FabricMC/fabric-example-mod: Example Fabric mod](https://github.com/FabricMC/fabric-example-mod) is what im starting with
						- need both the fabric installer (run with java -jar filename...) and [www.curseforge.com/minecraft/mc mods/fabric api/download/5897810](https://www.curseforge.com/minecraft/mc-mods/fabric-api/download/5897810)
						- 
						- 
						- paste from google keep:
						   - for local minecraft nn, consider implementing implicit learning so i can simply type a certain key when i come across a type of mob. then the neural net can learn what im looking at. the real problem comes in when we need to draw a bounding box. if we only click when looking at it, the neural net may not learn explicit feature that defined a mob (green color change defines a creeper). still useful though
                    - humanoid
                        - ![](../assets/optimus.gif)
                        - ![](../assets/helix.gif)
                    - dog
                        - ![](../assets/dog.gif)
                    - car
                        - ![](../assets/tesla-cv.gif)
                        - ![](../assets/fsd.gif)
                        - ![](../assets/waymo.gif)
                    - aircraft
                        - ![](../assets/autonomous-drone.png)
                        - ![](../assets/chester.gif)
                        - ![](../assets/fpv-accel.gif)
            - non-performance (reasoning) breakthroughs >= [transformers](https://arxiv.org/abs/1706.03762)
                - ![](../assets/attnisallyouneed.png)
            - other paradigms to make machines intelligent/learn
                - no [backpropagation](https://www.cs.utoronto.ca/~bonner/courses/2016s/csc321/readings/Learning%20representations%20by%20back-propagating%20errors.pdf)
                - no gradient descent
                - no loss functions
                - evolutionary algorithms
            - compute effect of using more powerful AIs to assist with AGI research
            - audio models
            - video models
            - AI agents
                - environments
                    - [minerl](https://minerl.readthedocs.io/en/latest/) & [minerl github](https://github.com/minerllabs/minerl)
                    - openai gym
                        - [openai gym github](https://github.com/openai/gym)
                        - [paper](https://arxiv.org/pdf/1606.01540)
                - agents performance
                    - [factorio systems engineering/design by hierarchical agents](https://arxiv.org/pdf/2502.01492)
                        - ![](../assets/factorio-agents.png)
                        - ![](../assets/factorio-table.png)
            - vision models
                - [yolo](https://arxiv.org/pdf/1506.02640) (you only look once)
                    - [yolov8](https://arxiv.org/pdf/2408.15857)
                    - [yolov11](https://github.com/ultralytics/ultralytics)
                    - [sota obj det models](https://github.com/Infatoshi/rt-detr-yolo-compare)
                - vision transformers (ViTs)
                    - [JARVIS-VLA (minecraft)](https://craftjarvis.github.io/JarvisVLA/)
                    - [ViT](https://arxiv.org/pdf/2010.11929)
                        - ![](../assets/vit-arch.png)
                    - [Swin Transformer](https://arxiv.org/pdf/2103.14030)

            - training / inference / CUDA
                - kernel generation:
                - kernel gen
	               - - [x.com/i/grok/share/dT7QTvzfbE2FW8l9vnXNJ8VHT](https://x.com/i/grok/share/dT7QTvzfbE2FW8l9vnXNJ8VHT)
					- ![](https://remnote-user-data.s3.amazonaws.com/Ji3sHEi5Ii-Sb_ni-nlsGXpcoODuyao0I5DpUqYpLqbVxp-KQK1M-OfRqbQPKWZeRnwpb70KdwZhAy27lvCZCUslcwGKNSowuf8s3RNPSAyVU_Q3-ytIY2RuiC2pf0o0.png)
					- MY QUESTIONS TO GROK:
					   - if i wanted to completely automate CUDA kernel generation with reasoning LLMs. my thought process is a bit scramble but looks like this: you ask the llm to generate a faster kernel with a text prompt (optional) along with some pytorch code. you take some input shape, do a bunch of operations and get an output shape to the final result. the idea is instead of seperately launching fast kernels, we fuse them into a SINGLE fast kernel. going lower level a model like deepseek-r1 would output some GPU code and we would have a verifier to ensure the gpu results match pytorch results. we would also have a speedup counter to see how much better the current optimization was from the past state. if the results dont match, we would feed it back through a loop where the prompt is customized to fix the script and encourage different approaches to figuring out why they dont match (error checking macros, stride/indexing errors, incorrect reductions, numerical instability, etc). once the results do match, we compare performance. performance is the other feedback loop which consists of a different set of prompts to optimize further (ill design this later). we could reformat the prompt each time for both ensuring correctness, and that the increase in performance during the optimization phase is done properly. ideally we would port each WORKING optimzation to different scripts with a specific name generated at random (adjective_noun). ANYWAYS... curious to hear your feedback on this
						- suppose this would be easier if we easier put more effort into a finetuning (SFT) a reasoning model on the CUDA compatibility matrix, device stats like ./deviceQuery from cuda-samples, etc. or use RAG, or just have a system of agents that places this info directly into the context window and figures out step by step (different agents have different system prompts as to how they take data and think up some good optimizations)
						- we actually would only need maybe flash attention2 and 3, thunderkittens, cutlass, and a couple (2) other projects. the rest could be RL because output verification and performance optimization are VERY EASY when it comes to crafting reward functions. like kernelbench from stanford for example: [https://arxiv.org/pdf/2502.10517v1](../%5B2502.10517v1%5D%20KernelBench_%20Can%20LLMs%20Write%20Efficient%20GPU%20Kernels_.md)
                  - [kernelbench](https://scalingintelligence.stanford.edu/KernelBenchLeaderboard/)
                  - [github](https://github.com/ScalingIntelligence/KernelBench)
                  - [arxiv](https://arxiv.org/pdf/2502.10517v1)
                - [guide for distributed computing (huggingface)](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
                - attention
                    - [Native Sparse Attention - DeepSeek](https://arxiv.org/pdf/2502.11089)
                    - [FlashAttention](https://arxiv.org/pdf/2205.14135)
                    - [FlashAttention2](https://arxiv.org/pdf/2307.08691)
                    - [FlashAttention3](https://arxiv.org/pdf/2407.08608)
                    - [SageAttention](https://arxiv.org/pdf/2410.02367)
                    - [Paged Attention](https://arxiv.org/pdf/2309.06180)
                    - [FlexAttention](https://arxiv.org/pdf/2412.05496)
                - general 
                    - [Triton](https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf)
					- [DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)
                    - [DeepSpeed](https://www.deepspeed.ai/)
                - inference only
                    - [KV Cache](https://arxiv.org/pdf/2211.05102)
                    - [Speculative Decoding](https://arxiv.org/pdf/2211.17192)
                    - [DeepSpeed Inference](https://arxiv.org/pdf/2207.00032)
                    - [vLLM](https://github.com/vllm-project/vllm)
                    - [llama.cpp](https://github.com/ggml-org/llama.cpp)
                    - [tensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
                    - [SGLang](https://arxiv.org/pdf/2312.07104)
                - optimizers
                    - [ZeRO](https://arxiv.org/pdf/1910.02054)


            - existing techniques
                - distillation
                    - token-level distillation
                    - online logit distillation
                - Mixture-of-Experts
                    - [Original MoE Paper](https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/jacobs1991moe.pdf)
                    - [Auxiliary-Loss-Free Load Balancing Strategy for MoEs](https://arxiv.org/pdf/2408.15664)
                - learning hacks to shorten training time
                    - [GrokFast](https://arxiv.org/pdf/2405.20233)
                - quantization (int4/fp4 optimal?)
                    - [k-bit scaling](https://arxiv.org/pdf/2212.09720)
                    - [The Era of 1-bit LLMs: All LLMs are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)
                - tokenization hacks
                    - [SpaceByte](https://arxiv.org/pdf/2404.14408)
                    - [SuperBPE](https://arxiv.org/pdf/2503.13423)
                - test-time compute (reasoning & think tokens)
                    - [DeepSeek-R1](https://arxiv.org/pdf/2501.12948)
                - reasoning in latent space
                    - [Quiet-STaR](https://arxiv.org/pdf/2403.09629)
                    - [Recurrent Depth Approach to Latent Reasoning](https://arxiv.org/pdf/2502.05171)
                - embeddings / pos enc
                    - [V-JEPA](https://arxiv.org/pdf/2404.08471)
                    - [Rotary Positional Embedding](https://arxiv.org/pdf/2104.09864)
                - RLHF (ppo/dpo)
                    - [Learning to Summarize from Human Feedback](https://arXiv.org/abs/2009.01325)
                    - [Deep Reinforcement Learning from Human Preferences](https://arXiv.org/abs/1706.03741)
                    - [Fine-Tuning Language Models from Human Preferences](https://arXiv.org/abs/1909.08593)
                    - [Training Language Models to Follow Instructions with Human Feedback](https://arXiv.org/abs/2203.02155)
                    - [Scaling Laws for Reward Model Overoptimization](https://arXiv.org/abs/2210.10760)
                    - [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arXiv.org/abs/2305.18290)
                - RL (in general)
                    - [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)
                - diffusion models
                    - [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233)
                    - [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)
                - synthetic data
                    - [Improving the Scaling Laws of Synthetic Data with Deliberate Practice](https://arxiv.org/pdf/2502.15588)
                    - [OpenAI VPT](https://arxiv.org/pdf/2206.11795)
                - base transformer additions
                    - [differential attention (for attention noise reduction)](https://arxiv.org/pdf/2410.05258)
                    - Kolmogorov-Arnold-Network (KAN)
                        - [KANs](https://arxiv.org/pdf/2404.19756)
                        - [KAN 2.0](https://arxiv.org/pdf/2408.10205)
                        - FastKANs
                            - [paper](https://arxiv.org/pdf/2405.06721)
                            - [code](https://github.com/ZiyaoLi/fast-kan)
                        - FasterKANs
                            - [code](https://github.com/AthanasiosDelis/faster-kan)
                    - normalizations
                        - [layernorm](https://arxiv.org/pdf/1607.06450)
                        - [rmsnorm](https://arxiv.org/pdf/1910.07467)
                        - [postnorm vs prenorm](https://arxiv.org/abs/1706.03762)
